{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Backpropagation in a Simple Neural Network \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the following code and get the scatter plot.\n",
    "\n",
    "```\n",
    "# generate and visualize Make-Moons dataset \n",
    "\n",
    "X, y = generate_data() \n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot is \n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79gy1fk6pnknt4bj30dc0a0406.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement actFun(self, z, type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the activation function based on their defination.\n",
    "\n",
    "```\n",
    "def actFun(self, z, type):\n",
    "        '''\n",
    "        actFun computes the activation functions\n",
    "        :param z: net input\n",
    "        :param type: Tanh, Sigmoid, or ReLU\n",
    "        :return: activations\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLMENT YOUR actFun HERE\n",
    "        if type == 'tanh':\n",
    "            act = np.tanh(z)\n",
    "        if type == 'sigmoid':\n",
    "            act = 1. / (1 + np.exp(-z))\n",
    "        if type == 'relu':\n",
    "            act = z * (z > 0)\n",
    "\n",
    "        return act\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Derive the derivatives of Tanh, Sigmoid, and ReLU\n",
    "\n",
    "#### Tanh function\n",
    "\n",
    "We know that $tanh = \\frac{1-e^{-2x}}{1+e^{-2x}}$, and now we will take derivative of this function.\n",
    "\n",
    "$\\frac{d tanh(x)}{dx} = \\frac{2e^{-2x}}{1+e^{-2x}} + \\frac{(1-e^{-2x})*2e^{-2x}}{(1+e^{-2x})^2} = \\frac{4e^{-2x}}{(1+e^{-2x})^2} = 1 - tanh^2(x)$\n",
    "\n",
    "#### Sigmoid function\n",
    "\n",
    "$\\frac{d sig(x)}{dx} = \\frac{e^{-x}}{(1 + e^{-x})^{-2}} = sig(x)(1- sig(x))$\n",
    "\n",
    "#### ReLU function\n",
    "\n",
    "obviously, the derivative of $\\mathbf{x}$ is, 1 for $x_i$ > 0 and 0 for $x_i = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement function diff\\_actFun(self, z, type)\n",
    "\n",
    "Use the derivative result in 2 to define the function.\n",
    "\n",
    "```\n",
    "    def diff_actFun(self, z, type):\n",
    "        '''\n",
    "        diff_actFun computes the derivatives of the activation functions wrt the net input\n",
    "        :param z: net input\n",
    "        :param type: Tanh, Sigmoid, or ReLU\n",
    "        :return: the derivatives of the activation functions wrt the net input\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLEMENT YOUR diff_actFun HERE\n",
    "        if type == 'tanh':\n",
    "            diff = 1 - np.tanh(z)**2\n",
    "        if type == 'sigmoid':\n",
    "            sig = 1 / (1 + np.exp(-z))\n",
    "            diff = sig * (1 - sig)\n",
    "        if type == 'relu':\n",
    "            diff = 1 * (z > 0) \n",
    "\n",
    "        return diff\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the function feedforward(self, X, actFun).\n",
    "\n",
    "Feedforward the model by feeding the model parameters.\n",
    "\n",
    "```\n",
    "def feedforward(self, X, actFun):\n",
    "        '''\n",
    "        feedforward builds a 3-layer neural network and computes the two probabilities,\n",
    "        one for class 0 and one for class 1\n",
    "        :param X: input data\n",
    "        :param actFun: activation function\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        # YOU IMPLEMENT YOUR feedforward HERE\n",
    "\n",
    "        self.z1 = self.W1 * X + self.b1\n",
    "        self.a1 = actFun(self.z1)\n",
    "        self.z2 = self.W2 * self.a1 + self.b2\n",
    "        exp_scores = np.exp(self.z2)\n",
    "        self.probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        return None\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the function calculate loss(self, X, y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since y is one-hot vector, we could just choose the label column to get the mulplication.\n",
    "\n",
    "```\n",
    " def calculate_loss(self, X, y):\n",
    "        '''\n",
    "        calculate_loss computes the loss for prediction\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return: the loss for prediction\n",
    "        '''\n",
    "        num_examples = len(X)\n",
    "        self.feedforward(X, lambda x: self.actFun(x, type=self.actFun_type))\n",
    "        # Calculating the loss\n",
    "\n",
    "        # YOU IMPLEMENT YOUR CALCULATION OF THE LOSS HERE\n",
    "        data_loss = -np.sum(np.log(self.probs)[np.arange(num_examples),\n",
    "                            y])/num_examples\n",
    "\n",
    "        # Add regulatization term to loss (optional)\n",
    "        data_loss += self.reg_lambda / 2 * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))\n",
    "        return (1. / num_examples) * data_loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Pass - Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Derive the gradients\n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "1 First, consider the gradient of $W_2$.\n",
    "\n",
    "The loss function is $L = -\\frac{1}{N}\\sum_{n = 1}^N y_n log \\hat{y}_n$, the inner dot product of $y_n$ and $\\hat{y}_n$, where $y_n$ is the one-hot vector of $n_{th}$ sample and $\\hat{y}_n$ is the output of $n_{th}$  sample. For computation ease, we only consider one sample, $l_1 = -y log \\hat{y}$. Now we have $\\frac{\\partial l_1}{\\partial w_1^{ij}} = \\sum_{k,l} \\frac{\\partial l_1}{\\partial \\hat{y}^k}\\frac{\\partial \\hat{y}^k}{\\partial z_2^l}\\frac{\\partial z_2^l}{\\partial w_1^{ij}}$, where ${\\partial z_2^l}$ represents the $l_{th}$ element of $z_2$.\n",
    "\n",
    "Obviously the $\\frac{\\partial l_1}{\\partial \\hat{y}^k} = \\frac{y^k}{\\hat{y}^k}$.\n",
    "\n",
    "Next, the gradient of softmax function:\n",
    "$\\frac{\\partial \\hat{y}^k}{\\partial z_2^l} =  \\begin{cases}-\\hat{y}^k\\hat{y}^l & k \\neq  l\\\\\\hat{y}^k(1-\\hat{y}^k) & k = l\\end{cases}$.\n",
    "\n",
    "To ease the computation, now we have $\\frac{\\partial l_1}{\\partial z_2^l} = \\sum_k \\frac{\\partial l_1}{\\partial \\hat{y}^k}\\frac{\\partial \\hat{y}^k}{\\partial z_2^l} = \\hat{y}^l - y^l$\n",
    "\n",
    "Now combine the third part, $\\frac{\\partial l_1}{\\partial w_2^{ij}} = (\\hat{y}^i - y^i)a_{1}^j$. Notice it is the outer product, we have $\\frac{\\partial l_1}{\\partial W_2} = (\\hat{y} - y)a_1^T$\n",
    "\n",
    "2 Consider the gradient of $b_2$, we can directly get $\\frac{\\partial l_1}{\\partial b_2} = (\\hat{y} - y)$.\n",
    "\n",
    "3 Next, to calculate the gradient of $W_1$, We have to go futher steps.\n",
    "\n",
    "$\\frac{\\partial l_1}{\\partial w_1^{ij}} =  \\sum_{l,k,h} \\frac{\\partial l_1}{\\partial z_2^l}\\frac{\\partial z_2^l}{\\partial a_1^k}\\frac{\\partial a_1^k}{\\partial z_1^h} \\frac{\\partial z_1^h}{\\partial w_2^{ij}} =  \\sum_{l,k,h}(\\hat{y}^l - y^l)w_2^{lk}\\frac{\\partial a_1^k}{{\\partial z_1^h}}\\frac{\\partial z_1^h}{\\partial w_1^{ij}}$, with $\\frac{\\partial a_1^k}{{\\partial z_1^h}} = \\begin{cases} a_1^h(1-a_1^h) & k = h\\\\0 & k \\neq h\\end{cases}$.\n",
    "\n",
    "Now we can sum this w.r.t k, that is $\\frac{\\partial l_1}{\\partial w_2^{ij}} =  \\sum_{l,h} (\\hat{y}^l - y^l)w_2^{lh} a_1^h(1-a_1^h)\\frac{\\partial z_1^h}{\\partial w_1^{i,j}} =  \\sum_{l} (\\hat{y}^l - y^l) w_2^{li}a_1^i(1- a_1^i)x_j$. The matrix form is $W_2^T(\\hat{y} -y).*a_1.*(1-a_1)x^T$, where $.*$ represents the element-wise product.\n",
    "\n",
    "4 Finally, the gradient of $b_1$ is $W_2^T(\\hat{y} -y).*a_1.*(1-a_1)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tanh**\n",
    "\n",
    "Notice the only different part between Tanh and Sigmoid is the gradient of activation funciton.\n",
    "The gradient of $W_2$ and $b_2$ are the same as sigmoid. While the gradient of tanh(z) is $1-tanh(z)^2$, the gradient of $W_1$ is $W^T(\\hat{y} -y).*(1-a_1^Ta_1)x^T$\n",
    "\n",
    "And the gradient of $b_1$ is  $W^T(\\hat{y} -y).*(1-a_1^Ta_1)$. \n",
    "\n",
    "**ReLu**\n",
    "\n",
    "$W_2$ and $b_2$ are the same as sigmoid. The gradient of $W_1$ is $W^T(\\hat{y} -y).*(1*(y>0))x^T$ and the gradient of $b_1$ is $W^T(\\hat{y} -y).*(1*(y>0))$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement the function backprop(self, X, y).\n",
    "\n",
    "Notice the data dimension of W and b are different from the equation, the codes are not strictly align with mathematics models, with some tranposition involved.\n",
    "\n",
    "```\n",
    "def backprop(self, X, y):\n",
    "        '''\n",
    "        backprop implements backpropagation to compute the gradients used to update the parameters in the backward step\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return: dL/dW1, dL/b1, dL/dW2, dL/db2\n",
    "        '''\n",
    "\n",
    "        # IMPLEMENT YOUR BACKPROP HERE\n",
    "        num_examples = len(X)\n",
    "        delta3 = self.probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        dW2 = 1/num_examples * np.matmul(np.transpose(self.a1),delta3)\n",
    "        db2 = 1/num_examples * np.sum(delta3, axis = 0)\n",
    "        dW1 = 1/num_examples * np.matmul(np.transpose(X),np.multiply(\n",
    "                np.matmul(delta3, np.transpose(self.W2)), \n",
    "                self.diff_actFun(self.a1, self.actFun_type)))\n",
    "        db1 = 1/num_examples * np.sum(\n",
    "                np.multiply(np.matmul(delta3, np.transpose(self.W2)), \n",
    "                            self.diff_actFun(self.a1, self.actFun_type)),\n",
    "                            axis = 0)\n",
    "        return dW1, dW2, db1, db2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time to have fun - Training!\n",
    "\n",
    "### Compare the three activation functions\n",
    "\n",
    "\n",
    "Now, we can train the model by runnning the three_layer_neural_network.py. First, we explore the difference between different activation functions.\n",
    "\n",
    "**Sigmoid:**\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tKfTcly1fjwc6kv2p7j30hs0dcgng.jpg)\n",
    "\n",
    "**Tanh**\n",
    "\n",
    "![](https://ws2.sinaimg.cn/large/006tKfTcly1fjwc3kwt8zj30hs0dc40b.jpg)\n",
    "\n",
    "**ReLu**\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcly1fjwc9mvamyj30hs0dcabv.jpg)\n",
    "\n",
    "Obviously, Sigmoid and Tanh function with one hidden layer are both acting similarly as linear classfication which will change with the hidden units increasing, while the ReLu activation gives us an obvious unliear classification. The loss of ReLu method is the less than the other two methods, with the next lowest Tanh activation function. This can be seen from the graph, there is more blue points in red part in Sigmoid activation model than the other two.\n",
    "\n",
    "### Explore the Tanh activation function\n",
    "\n",
    "Next, we will change the number of hidden units to explore the differences of models.\n",
    "\n",
    "**5 hidden units**\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tKfTcgy1fjxen98t0dj30hs0dcabv.jpg)\n",
    "\n",
    "**8 hidden units**\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tKfTcgy1fjxenxre7dj30hs0dcwgb.jpg)\n",
    "\n",
    "**50 hidden units**\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tKfTcly1fjxevzl1l9j30hs0dcwgb.jpg)\n",
    "\n",
    "\n",
    "We can see that lower hidden size gives us a more general one, while higher hidden size makes the classfication more precise. However, the percise is not a good thing, with high risk of overfitting. Generally, the loss is decreasing with the hidden units increasing, which can been seen as more information being captured with higher hidden units. The accurate of model with 50 hidden units is higher than model with 5 hidden units. I believe if we train more steps, the accurate will be higher, but with higher risk of overfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even more fun - Training a deeper network!!\n",
    "\n",
    "\n",
    "### Rewritten the neural network class\n",
    "\n",
    "To train a deep neural network with multiple hidden layers, it is more convenient to construct a new class to forward and backward go through each layer.\n",
    "The Layer class is defined as below:\n",
    "\n",
    "```\n",
    "\n",
    "import numpy as np\n",
    "from configs import configs\n",
    "\n",
    "config = configs()\n",
    "\n",
    "class Layer():\n",
    "    \n",
    "    def __init__(self, layer_id, actFun):\n",
    "\n",
    "        self.actFun = actFun\n",
    "        self.layer_id = layer_id\n",
    "        \n",
    "    \n",
    "        \n",
    "    def feedforward(self, input_, W, b):\n",
    "        \n",
    "        if self.layer_id < config.nn_layers - 1:       \n",
    "            self.z = np.matmul(input_, W) + b\n",
    "            self.a = self.actFun(self.z)\n",
    "        \n",
    "        if self.layer_id == config.nn_layers - 1:\n",
    "            self.z = np.matmul(input_, W) + b\n",
    "            exp_scores = np.exp(self.z)\n",
    "            self.a = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        return self.a\n",
    "        \n",
    "    def backprop(self, input_, num_examples, delta):\n",
    "            \n",
    "            dW = 1/num_examples * np.dot(np.transpose(input_),delta)\n",
    "            db = 1/num_examples * np.sum(delta, axis = 0, keepdims = True)\n",
    "            \n",
    "            return dW, db\n",
    "\n",
    "```\n",
    "\n",
    "We define two main functions: feedforward and backprop, with parameters feed into these functions. The parameters are updated in the multiple neural network class, while Layer class is only responsible for calculating.\n",
    "\n",
    "Next, we need to reedit the feedforward, backprop, calculate_loss and fit_model functions in the neural network class. \n",
    "\n",
    "1.The feedforward codes are like below:\n",
    "\n",
    "```\n",
    "def feedforward(self):\n",
    "        '''\n",
    "        update the neurons in all layers\n",
    "        '''\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.neurons[i+1] = layer.feedforward(self.neurons[i], self.W[i], self.b[i])\n",
    "\n",
    "        return None\n",
    "```\n",
    "\n",
    "We only need to pass the parameters to the Layer.feedforward(), much more clear version.\n",
    "\n",
    "2.Backprop\n",
    "\n",
    "```\n",
    "def backprop(self):\n",
    "        \n",
    "        for i, layer in enumerate(self.layers[::-1]):\n",
    "            \n",
    "            index = config.nn_layers - 1 - i\n",
    "            \n",
    "            self.dW[index], self.db[index] = layer.backprop(self.neurons[index],\n",
    "                   len(self.X), self.delta[index])\n",
    "        \n",
    "        return None\n",
    "\n",
    "```\n",
    "\n",
    "Also, feed the parameters to Layer.backprop(). Notice, the backprop is updated in reverse order, from the last hidden layer to first hidden layer.\n",
    "\n",
    "3.calculate_loss\n",
    "\n",
    "```\n",
    "def calculate_loss(self):\n",
    "        '''\n",
    "        calculate_loss computes the loss for prediction\n",
    "        :param X: input data\n",
    "        :param y: given labels\n",
    "        :return: the loss for prediction\n",
    "        '''\n",
    "        num_examples = len(self.X)\n",
    "        self.feedforward()\n",
    "        # Calculating the loss\n",
    "\n",
    "        data_loss = -np.sum(np.log(self.neurons[-1])[np.arange(num_examples),\n",
    "                            self.y])/num_examples\n",
    "\n",
    "        # Add regulatization term to loss (optional)\n",
    "        data_loss += config.reg_lambda / 2 * np.sum([np.sum(np.square(W)) for W \n",
    "                                                   in self.W] )\n",
    "        return (1. / num_examples) * data_loss\n",
    "\n",
    "```\n",
    "\n",
    "Here, I changed the data_loss slightly to update the name. Also the regulization part is changed.\n",
    "\n",
    "4.fit_model\n",
    "\n",
    "```\n",
    "def fit_model(self, epsilon=0.01, num_passes=20000, print_loss=True):\n",
    "        \n",
    "        # Gradient descent.\n",
    "        for j in np.arange(0, num_passes):\n",
    "            # Forward propagation\n",
    "            self.feedforward()\n",
    "            # Updata deltas\n",
    "            self.neurons[-1][range(len(self.X)), self.y] -= 1\n",
    "            self.delta[-1] = np.array(self.neurons[-1])\n",
    "            for i in np.arange(config.nn_layers)[-2::-1]:\n",
    "                self.delta[i] = np.multiply(np.dot(self.delta[i+1], self.W[i+1].T), \n",
    "                          self.diff_actFun(self.neurons[i+1], config.actFun_type))\n",
    "            \n",
    "            # Backpropagation\n",
    "            self.backprop()\n",
    "\n",
    "            # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "            self.dW = [dw + config.reg_lambda * w for w, dw in zip(self.W, self.dW)]\n",
    "\n",
    "            # Gradient descent parameter update\n",
    "            self.W = [w - epsilon * dw for w, dw in zip(self.W, self.dW)]\n",
    "            self.b = [b - epsilon * db for b, db in zip(self.b, self.db)]\n",
    "            \n",
    "            if print_loss and j % 1000 == 0:\n",
    "                print(\"Loss after iteration {}:{}\".format(j, self.calculate_loss()))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The fit_model part is rewritten to include the list of deltas, which is needed in backprop function to calculate gradients.\n",
    "\n",
    "### Results\n",
    "\n",
    "This plot is trained with hidden_layer = 4. Obviously, the prediction fits the points nearly perfect. Then we know the more hidden layers, the more accurate the predictions are. However, it is extremely possible to overfit.\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tKfTcly1fk5zew8fx9j30hs0dcgnc.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Simple Deep Convolutional Network on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Build and Train a 4-layer DCN\n",
    "\n",
    "2.Complete the weight and bias function\n",
    "\n",
    "```\n",
    "def weight_variable(shape):\n",
    "    '''\n",
    "    Initialize weights\n",
    "    :param shape: shape of weights, e.g. [w, h ,Cin, Cout] where\n",
    "    w: width of the filters\n",
    "    h: height of the filters\n",
    "    Cin: the number of the channels of the filters\n",
    "    Cout: the number of filters\n",
    "    :return: a tensor variable for weights with initial values\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR WEIGHT_VARIABLE HERE\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    W = tf.Variable(initial)\n",
    "    return W\n",
    "\n",
    "def bias_variable(shape):\n",
    "    '''\n",
    "    Initialize biases\n",
    "    :param shape: shape of biases, e.g. [Cout] where\n",
    "    Cout: the number of filters\n",
    "    :return: a tensor variable for biases with initial values\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR BIAS_VARIABLE HERE\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    b = tf.Variable(initial)\n",
    "    return b\n",
    "    \n",
    "def conv2d(x, W):\n",
    "    '''\n",
    "    Perform 2-D convolution\n",
    "    :param x: input tensor of size [N, W, H, Cin] where\n",
    "    N: the number of images\n",
    "    W: width of images\n",
    "    H: height of images\n",
    "    Cin: the number of channels of images\n",
    "    :param W: weight tensor [w, h, Cin, Cout]\n",
    "    w: width of the filters\n",
    "    h: height of the filters\n",
    "    Cin: the number of the channels of the filters = the number of channels of images\n",
    "    Cout: the number of filters\n",
    "    :return: a tensor of features extracted by the filters, a.k.a. the results after convolution\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR CONV2D HERE\n",
    "    h_conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return h_conv\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    '''\n",
    "    Perform non-overlapping 2-D maxpooling on 2x2 regions in the input data\n",
    "    :param x: input data\n",
    "    :return: the results of maxpooling (max-marginalized + downsampling)\n",
    "    '''\n",
    "\n",
    "    # IMPLEMENT YOUR MAX_POOL_2X2 HERE\n",
    "    h_max = tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "    return h_max\n",
    "\n",
    "```\n",
    "\n",
    "3.Build the network\n",
    "\n",
    "Next, in the main function, we build the network\n",
    "\n",
    "```\n",
    "\n",
    "# FILL IN THE CODE BELOW TO BUILD YOUR NETWORK\n",
    "\n",
    "    # placeholders for input data and input labeles\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "    # reshape the input image\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "    # first convolutional layer\n",
    "    W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "    b_conv1 = bias_variable([32])\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # second convolutional layer\n",
    "    W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "    b_conv2 = bias_variable([64])\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # densely connected layer\n",
    "    W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "    b_fc1 = bias_variable([1024])\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # dropout\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # softmax\n",
    "    W_fc2 = weight_variable([1024, 10])\n",
    "    b_fc2 = bias_variable([10])\n",
    "    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "4.Set up training\n",
    "\n",
    "```\n",
    "# FILL IN THE FOLLOWING CODE TO SET UP THE TRAINING\n",
    "\n",
    "    # setup training\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "5.Run training.\n",
    "\n",
    "We run the graph by creating a session, and we also write summary and graph. The final test accuracy is 0.9869.\n",
    "\n",
    "\n",
    "6.Visualize training\n",
    "\n",
    "In terminal, ~ py$ tensorboard --logdir='/Users/py/Python/comp_576/results'\n",
    "\n",
    "We get the graph:\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcgy1fk6723iv3wj31kw0qewrr.jpg)\n",
    "\n",
    "Also, we could track the loss path:\n",
    "\n",
    "![](https://ws2.sinaimg.cn/large/006tNc79gy1fk6mt75ctdj30kb08twfk.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) More on Visualizing Your Training\n",
    "\n",
    "Now, we go a step further on how to monitor a set of variables during the training. As in the tutorial of TensorBoard, let's define a function to summarize statistics of variable.\n",
    "\n",
    "```\n",
    "def variable_summaries(var):\n",
    "\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "```\n",
    "\n",
    "Apply this function on weights, bias, net inputs, activations after Relu at each layer, activations after max-pooling at each layer. Keep using tf.summary.merge_all(), then all the summaries of these variables are created on TensorBoard. \n",
    "\n",
    "```\n",
    "# write summaries\n",
    "    variable_summaries(W_conv1)\n",
    "    variable_summaries(W_conv2)\n",
    "    variable_summaries(W_fc1)\n",
    "    variable_summaries(W_fc2)\n",
    "    variable_summaries(b_fc2)\n",
    "    variable_summaries(b_fc1)\n",
    "    variable_summaries(b_conv2)\n",
    "    variable_summaries(b_conv1)\n",
    "    variable_summaries(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "    variable_summaries(conv2d(x_image, W_conv1) + b_conv1)\n",
    "    variable_summaries(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    variable_summaries(h_conv1)\n",
    "    variable_summaries(h_conv2)\n",
    "    variable_summaries(h_fc1)\n",
    "    variable_summaries(h_pool1)\n",
    "    variable_summaries(h_pool2)\n",
    "\n",
    "```\n",
    "\n",
    "The statistics of weight at 1st layer is like follow:\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79gy1fk6my8ugi5j30kb0degoo.jpg)\n",
    "\n",
    "There is also histogram for weight:\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79gy1fk6mpf9kedj30dk0a0gqd.jpg)\n",
    "\n",
    "The statistics of bias at 1st layer is also attached:\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79gy1fk6mwn0qbfj30kb0dugp9.jpg)\n",
    "\n",
    "And the histogram of bias at 1st layer:\n",
    "\n",
    "![](https://ws2.sinaimg.cn/large/006tNc79gy1fk6mqryoiqj30dk093myh.jpg)\n",
    "\n",
    "The mean of weight and bias is reducing, while the standard error is increasing, which means less robost.\n",
    "\n",
    "Further, create a new FileWrite for test error. Every 1100 steps, we write the summary for test errors, with test set feed into session.\n",
    "\n",
    "```\n",
    "test_writer = tf.summary.FileWriter(test_dir)\n",
    "\n",
    "if i % 1100 == 0 or i == max_step:\n",
    "            summary_test_err = sess.run(summary_loss, feed_dict = {\n",
    "                    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0})\n",
    "            test_writer.add_summary(summary_test_err, i)\n",
    "            test_writer.flush()\n",
    "            checkpoint_file = os.path.join(result_dir, 'checkpoint')\n",
    "            saver.save(sess, checkpoint_file, global_step=i)\n",
    "            \n",
    "```\n",
    "The monitered test error is like below:\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tNc79gy1fk6mukveomj30kb08st9m.jpg)\n",
    "\n",
    "We can see the loss of test set is super low after 1000 steps.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Time for More Fun!!!\n",
    "\n",
    "Here, I explored the model by trying with diï¬€erent nonlinearities (tanh), initialization techniques (Xavier...) and training algorithms (SGD, Momentum-based Methods). To accerlate the process, I reduce checking point steps into 200 and total steps into 1100.\n",
    "\n",
    "For activation, try use `tf.nn.tanh` \n",
    "\n",
    "```\n",
    "h_conv1 = tf.nn.tanh(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "```\n",
    "For training algorithm, try `tf.train.GradientDescentOptimizer `\n",
    "\n",
    "```\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "```\n",
    "\n",
    "For initialization techniques, try use `tf.contrib.layers.xavier_initializer()` for weights, but unchaged initilizer for bias. The function parameters are slightly different to name the W.\n",
    "\n",
    "```\n",
    "def weight_variable(shape, name):\n",
    "\n",
    "    initial = tf.contrib.layers.xavier_initializer()\n",
    "    W = tf.get_variable(name = name, shape = shape, initializer = initial)\n",
    "    return W\n",
    "\n",
    "```\n",
    "\n",
    "Now, retrain the model, notice the test accuracy for 5500 steps is 0.6585000157356262, which is significantly lower than last model. Besides, it moves slowly after we changed to `tf.train.GradientDescentOptimizer` method. Again, we could moniter the variables on TensorBoard.\n",
    "\n",
    "1.First, the training loss\n",
    "![](https://ws2.sinaimg.cn/large/006tNc79gy1fk6otmyynzj30kb08y402.jpg)\n",
    "\n",
    "2.Next, the test loss\n",
    "\n",
    "3.I also list the statistics of weights at 1st layer to compare:\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79gy1fk6pbemucfj30g90a6q5h.jpg)\n",
    "\n",
    "The moving is hazard at first hundreds steps, and the mean is increasing with steps further. The variance is also being larger and larger. In general, this model does not perform as well as the old one. I suppose this is due to the activation function.\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79gy1fk6p9sk1ugj30f80a8dlz.jpg)\n",
    "\n",
    "The distribution of weight is following the uniform, which is default distribution in `tf.contrib.layers.xavier_initializer()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The github page is:  https://github.com/Keyspan/comp576"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
